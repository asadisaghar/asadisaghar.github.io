---
layout: post
title: Predictive maintenance using C-MAPSS dataset
---

Lately, I've gotten interested in predictive maintenance and wonder how close are we in replacing preventive maintenance with predictive models. The more I read/thought about it the more I realized how much the answer hinges on the practice of keeping record of all piece replacements and maintenance, and engineering logs etc. and the whole thing became more and more overwhelming... Until I came across [this](https://ti.arc.nasa.gov/tech/dash/pcoe/prognostic-data-repository/#turbofan) -admittedly over simplified- set of -simulated- data and decided to start playing with it. The details of the simulations and some contextual information about the data are explained in a [paper](https://www.google.no/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0ahUKEwis05bV4LHWAhWKfRoKHZabAakQFggoMAA&url=https%3A%2F%2Fti.arc.nasa.gov%2Fpublications%2F154%2Fdownload%2F&usg=AFQjCNELnrsSWOvSTQlF39WXmZ9cF9ka5g) that I have only browsed very briefly and even now, have no idea what each of the columns I've worked with mean, but in a way this is part of the fun too.

#### About CMAPSS dataset
![Data description](images/2017-9-19-pdm_data_description.jpg)

BLAAAH

According to the README that comes with the dataset, the format of training and test sequences (for each engine with a separate id) looks like the schematics bellow. This also implies that we have more observations of systems that are critically close to their failure than those farther away from it. This could be our curse or blessing, depending on what we are interested in. If we are interested in spotting a failing system as soon as possible (but possibly with a larger margine of accuracy), this is not how we want our training and test set to  look like. However, if we are interested in learning the exact beahviour of the system after the first fault and the growth of the system failure, we could embrace this training set. Although the test set then may not be perfect for us. This will get clearer if we look at the actual distribution of RUL values (in cycle) for the training and test sets.

![Unscaled RUL](images/2017-9-19-pdm_RUL_cycle.png)

This relationship between lables of the training and test set looks a bit troublesome, how could we expect our model that is trained on the red plot, return reliable prediction of system on the green?

#### Regression or Classification

#### Preprocessing
- [Preprocessing notebook](https://github.com/asadisaghar/PdM-C-MAPSST/blob/master/play/publish/preprocessing.ipynb)

#### Supervised vs. unsupervised
- [Unsupervised classification notebook](https://github.com/asadisaghar/PdM-C-MAPSST/blob/master/play/publish/unsupervised_classification.ipynb)
- [Supervised classification notebook](https://github.com/asadisaghar/PdM-C-MAPSST/blob/master/play/publish/supervised_classification.ipynb) including KNN, a shallow fully-connected feed forward network, and a shallow hour-glass architecture (because it's fun!)
- [LSTM classification](https://github.com/asadisaghar/PdM-C-MAPSST/blob/master/play/publish/LSTM_classification.ipynb) probably a bit over-kill for this particular dataset but fun to play with anyway.
